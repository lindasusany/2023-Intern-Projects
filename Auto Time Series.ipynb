{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b58c1d70-96eb-42fe-b33a-ba07bf017475",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a77e512-2b51-4e2b-b5f8-8d09c08e9846",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -i https://pypi.org/simple tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bdee28d-91b4-4cbe-b8d8-20774f293eec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9657d071-9479-461a-9d2c-3bb389d99cb9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install auto_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25e08476-a0a2-43a9-9538-42216c9f1a77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.graphics.tsaplots import plot_acf,plot_pacf\n",
    "from numpy import log\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from pyspark.sql import SparkSession\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from hyperopt import hp, fmin, tpe, Trials\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from auto_ts import auto_timeseries \n",
    "#from autots import AutoTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb1c2273-c029-4ef3-872d-fd147d8cb5e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Import dataset\n",
    "data = spark.table('crm_ah.msdyn_purchaseorder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3f52d1b-3515-4b5e-bfcb-7a24ec4859d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "FS = spark.table('crm_shared_entity.cmhc_fundingsource')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de038aae-38df-4c04-814a-5737c35b60b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cmhc_budgetallocationdate:timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f6897d5-e2ba-45ed-bb6d-fc7ea96293ba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "program one-many client file (CRM file) one-one advance one-one disburesement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9b70e39-46cb-4147-832a-315c02f2ab72",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "No1. task \n",
    "predict the count of PO in the future in daily frequency\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec0cb2d0-d01e-4153-9725-a178b5d72ba0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "No2. task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91942cf0-f274-4db5-b5b8-9301f55779ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "selected_data_spark = data.select('msdyn_totalamount','createdon','cmhc_budgetallocationdate')\n",
    "ts_pandas = selected_data_spark.toPandas()\n",
    "ts_pandas.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dc308d0-75de-48cc-b1bf-4584ea3631fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(ts_pandas.dtypes)\n",
    "ts_pandas['msdyn_totalamount'] = ts_pandas['msdyn_totalamount'].astype(float)\n",
    "ts_pandas_d = ts_pandas\n",
    "ts_pandas_d['createdon'] = pd.to_datetime(ts_pandas['createdon'])\n",
    "#ts_pandas_d['createdon'] = ts_pandas_d['createdon'].dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "ts_pandas_d.set_index('createdon', inplace=True)\n",
    "df_daily = ts_pandas_d.resample('D').sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dba0636a-2a06-4df0-8c7b-706089a4c567",
     "showTitle": true,
     "title": "Visualize Seasonality"
    }
   },
   "outputs": [],
   "source": [
    "#plot the data\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15,7\n",
    "ts_pandas.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5261b12d-7f2f-45f3-ac65-38d6576a702c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "decomposition = sm.tsa.seasonal_decompose(df_daily['msdyn_totalamount'], period=121)\n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residuals = decomposition.resid\n",
    "plt.figure(figsize=(30,10))\n",
    "plt.subplot(411)\n",
    "plt.plot(df_daily['msdyn_totalamount'],label='Original')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xticks(rotation = 45)\n",
    "plt.subplot(412)\n",
    "plt.plot(trend,label='Trend')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xticks(rotation = 45)\n",
    "plt.subplot(413)\n",
    "plt.plot(seasonal,label='Seasonality')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xticks(rotation = 45)\n",
    "plt.subplot(414)\n",
    "plt.plot(residuals,label='Residuals')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xticks(rotation = 45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c45cc6d-0e5a-4cd7-b1ac-5902dacbb6de",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "pre program trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b34476cd-477a-475c-b851-55bb2dd8464d",
     "showTitle": true,
     "title": "Stationary: Differencing Determination"
    }
   },
   "outputs": [],
   "source": [
    "result_d = adfuller(df_daily.msdyn_totalamount)\n",
    "print(\"Daily ADF Statistics : %f\" % result_d[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b915242f-d3c4-4dd3-86e8-65efea3230e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_acf(df_daily.msdyn_totalamount,lags=10)\n",
    "plot_pacf(df_daily.msdyn_totalamount,lags=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50acff57-cad6-4586-9c9e-56cd5bde0d2e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#df_daily.reset_index(inplace=True)\n",
    "# Split the dataset into train and test sets\n",
    "train_size_d = int(len(df_daily) * 0.8)  # 80% for training, adjust as needed\n",
    "train_data_d = df_daily[:train_size_d]\n",
    "test_data_d = df_daily[train_size_d:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e012fac-2bd4-4885-abef-c9e1148e080e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#build a arima model using the parameters gained from previous steps\n",
    "model_d = ARIMA(train_data_d['msdyn_totalamount'], order=(6, 7, 6))\n",
    "model_d = model_d.fit()\n",
    "forecast_steps_d = len(test_data_d)\n",
    "forecast_values_d = model_d.forecast(steps=forecast_steps_d)\n",
    "actual_values_d = test_data_d['msdyn_totalamount']\n",
    "# Calculate the RMSE\n",
    "rmse_d = np.sqrt(mean_squared_error(actual_values_d, forecast_values_d))\n",
    "print(\"RMSE(D):\", rmse_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16cff293-1a60-4506-b340-5faf610923f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Use Automatic Method\n",
    "import pmdarima as pm\n",
    "# Fit the auto ARIMA model\n",
    "opt_model_d = pm.auto_arima(train_data_d['msdyn_totalamount'], seasonal=False, error_action='ignore', suppress_warnings=True)\n",
    "# Generate the forecast\n",
    "opt_forecast_d = opt_model_d.predict(n_periods=len(test_data_d))\n",
    "# Calculate the RMSE\n",
    "rmse_d = np.sqrt(np.mean((test_data_d['msdyn_totalamount'] - opt_forecast_d) ** 2))\n",
    "# Print the best model and its corresponding RMSE\n",
    "print(\"Best Daily Model: ARIMA\", opt_model_d.order)\n",
    "print(\"Best Daily RMSE:\", rmse_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ca7c971-d3dc-481c-8384-2d18cf906818",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    order = (params['p'], params['d'], params['q'])\n",
    "    model = ARIMA(train_data_d['msdyn_totalamount'])\n",
    "    model_fit = model.fit()\n",
    "    forecast = model_fit.forecast(steps=len(test_data_d))\n",
    "    predictions = np.maximum(forecast, 0)\n",
    "    mse = mean_squared_error(test_data_d['msdyn_totalamount'], predictions)\n",
    "    return mse\n",
    "space = {\n",
    "    'p': hp.quniform('p', 1, 20, 1),\n",
    "    'd': hp.quniform('d', 1, 5, 1),\n",
    "    'q': hp.quniform('q', 1, 20, 1)\n",
    "}\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=100, trials=trials)\n",
    "best_p = int(best['p'])\n",
    "best_d = int(best['d'])\n",
    "best_q = int(best['q'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e193bc3-f566-4417-9f14-f17d8353545a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Train the optimal model with the best hyperparameters\n",
    "optimal_model = ARIMA(train_data_d['msdyn_totalamount'], order=(best_p, best_d, best_q))\n",
    "optimal_model_fit = optimal_model.fit()\n",
    "# Make predictions on the test data\n",
    "forecast = optimal_model_fit.forecast(steps=len(test_data_d))\n",
    "predictions = np.maximum(forecast, 0)\n",
    "# Calculate RMSE\n",
    "mse = mean_squared_error(test_data_d[\"msdyn_totalamount\"], predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"Optimal ARIMA Model:\")\n",
    "print(optimal_model_fit.summary())\n",
    "print(\"RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31c619f6-a618-412f-8c0b-cad152a8c581",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_data_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf7dbc0e-ffc4-4557-95f6-294a236d6a0e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "last_date_d = test_data_d['createdon'].iloc[-1]\n",
    "future_dates = pd.date_range(start=last_date_d, periods=30, freq='D')\n",
    "future_dates = future_dates.shift(1, freq='D')\n",
    "n_forecast_steps = len(future_dates)\n",
    "n_forecast = optimal_model_fit.predict(steps=n_forecast_steps)\n",
    "n_forecast = np.maximum(n_forecast, 0)\n",
    "n_forecast = n_forecast[:n_forecast_steps]\n",
    "forecast_data = pd.DataFrame({'createdon': future_dates, 'total disbursement amount': n_forecast})\n",
    "forecast_data.reset_index(drop=True, inplace=True)\n",
    "print(forecast_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4a8daf6-9393-436e-80ae-a68d2e4a8f6b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "merged_dates_d = pd.concat([test_data_d['createdon'], forecast_data['createdon']], ignore_index=True)\n",
    "merged_forecast_d = pd.concat([opt_forecast_d, forecast_data['total disbursement amount']], ignore_index=True)\n",
    "confidence_level = 0.90\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.plot(test_data_d['createdon'], test_data_d['msdyn_totalamount'], label='Actual')\n",
    "ax.plot(merged_dates_d, merged_forecast_d, label='Forecast')\n",
    "lower_bound_d = n_forecast - (rmse * confidence_level)\n",
    "upper_bound_d = n_forecast + (rmse * confidence_level)\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Total Disbursement Amount')\n",
    "ax.set_title('Forecast with Confidence Interval')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be461254-c1ed-4063-a458-9a6febdb4bcf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Specify the column names for timestamp and target variable\n",
    "timestamp_column = 'createdon'\n",
    "target_column = 'msdyn_totalamount'\n",
    "\n",
    "# Set the frequency of the time series data to hourly\n",
    "frequency = 'D'\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_size = int(len(df_daily) * 0.8)  # 80% for training, adjust as needed\n",
    "train_data = df_daily[:train_size]\n",
    "test_data = df_daily[train_size:]\n",
    "\n",
    "merged_dates_d = pd.concat([test_data['createdon'], forecast_data['createdon']], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed415a11-5a9d-4ce7-9452-002377aadb27",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Specify the column names for timestamp and target variable\n",
    "timestamp_column = 'createdon'\n",
    "target_column = 'msdyn_totalamount'\n",
    "\n",
    "# Set the frequency of the time series data to hourly\n",
    "frequency = 'D'\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_size = int(len(df_daily) * 0.8)  # 80% for training, adjust as needed\n",
    "train_data = df_daily[:train_size]\n",
    "test_data = df_daily[train_size:]\n",
    "\n",
    "# Run the AutoTS model to automatically generate time series forecasts\n",
    "model = auto_timeseries(\n",
    "    score_type='rmse',  # Specify the evaluation metric for model selection (optional)\n",
    "    forecast_period=len(test_data),  # Number of steps to forecast (same as test data length)\n",
    "    time_interval=frequency,  # Set the frequency of the time series data to hourly\n",
    "    non_seasonal_pdq=None,  # Specify the non-seasonal order (p,d,q) of the ARIMA model (optional)\n",
    "    seasonal_PDQ=None,  # Specify the seasonal order (P,D,Q,s) of the SARIMA model (optional)\n",
    "    model_type='best',  # Specify the model type or use 'best' for automatic model selection\n",
    "    verbose=2  # Set verbosity level\n",
    ")\n",
    "\n",
    "# Fit the AutoTS model to the training data\n",
    "model.fit(traindata=train_data, ts_column=timestamp_column, target=target_column)\n",
    "\n",
    "# Get the summary of the optimal model\n",
    "summary = model.get_leaderboard()\n",
    "\n",
    "# Print the summary\n",
    "print(summary)\n",
    "\n",
    "# Generate forecasts for the test data\n",
    "forecast = model.predict(testdata=forecast_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc6a6f51-0629-4b21-9755-f63822e7b4b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "848201b1-3b2e-41b2-b841-efe2cd33acca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Specify the column names for timestamp and target variable\n",
    "timestamp_column = 'createdon'\n",
    "target_column = 'msdyn_totalamount'\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_size = int(len(df_daily) * 0.8)  # 80% for training, adjust as needed\n",
    "train_data = df_daily[:train_size]\n",
    "test_data = df_daily[train_size:]\n",
    "frequency = 'D'\n",
    "\n",
    "# Run the AutoTS model to automatically generate time series forecasts\n",
    "model = AutoTS(\n",
    "    forecast_length=len(test_data),  # Number of steps to forecast (same as test data length)\n",
    "    frequency='D',  # Set the frequency of the time series data\n",
    "    prediction_interval=0.95,\n",
    "    ensemble=None,\n",
    "    models_mode='deep',\n",
    "    model_list = 'univariate',# or ['ARIMA','ETS']\n",
    "    max_generations=10,\n",
    "    num_validations=3,\n",
    "    no_negatives=True,\n",
    "    n_jobs='auto'\n",
    ")\n",
    "\n",
    "# Fit the AutoTS model to the training data\n",
    "model.fit(train_data)\n",
    "\n",
    "# Get the summary of the optimal model\n",
    "summary = model.get_leaderboard()\n",
    "\n",
    "# Print the summary\n",
    "print(summary)\n",
    "\n",
    "# Generate forecasts for the test data\n",
    "test_df = model.predict(testdata=test_data)\n",
    "\n",
    "# Print the forecasted values\n",
    "print(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7909af7-8fef-4f84-8af8-f9b7a0f3a153",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tensorflow\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "# Preprocess the data\n",
    "scaler = MinMaxScaler()\n",
    "df_daily['msdyn_totalamount'] = scaler.fit_transform(df_daily['msdyn_totalamount'].values.reshape(-1, 1))\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_size = int(len(df_daily) * 0.8)  # 80% for training, adjust as needed\n",
    "train_data = df_daily[:train_size]\n",
    "test_data = df_daily[train_size:]\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=50, activation='relu', input_shape=(1, 1)))\n",
    "model.add(Dense(units=1))\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Prepare the training data\n",
    "X_train = np.array(train_data['createdon']).reshape(-1, 1, 1)\n",
    "y_train = np.array(train_data['amount_scaled'])\n",
    "\n",
    "# Train the LSTM model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=1)\n",
    "\n",
    "# Prepare the test data\n",
    "X_test = np.array(test_data['createdon']).reshape(-1, 1, 1)\n",
    "y_test = np.array(test_data['amount_scaled'])\n",
    "\n",
    "# Generate predictions on the test data\n",
    "predictions_scaled = model.predict(X_test)\n",
    "\n",
    "# Inverse scale the predictions\n",
    "predictions = scaler.inverse_transform(predictions_scaled)\n",
    "\n",
    "# Create a DataFrame for the predictions\n",
    "forecast_df = pd.DataFrame({'createdon': test_data['createdon'], 'amount': predictions.flatten()})\n",
    "\n",
    "# Print the forecasted values\n",
    "print(forecast_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21cc75f7-9798-4fbf-982f-bd8c9828f5d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Task 3 (1)",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
